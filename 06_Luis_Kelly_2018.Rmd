---
title: "Correlation And Regression Homework"
author: Kelly Luis
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggplot2)
library(broom)
library(modelr)
```

Note: Datasets are available at http://whitlockschluter.zoology.ubc.ca/data so you don't have to type anything in (and have to load it!)  

## 1. Correlation - W&S Chapter 16

###Questions 15
```{r C16_Q15, echo = TRUE}
# Download Data
gray <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter16/chap16q15LanguageGreyMatter.csv"))

# A. Display association between the two variables in a scatter plot
ggplot(data = gray, aes(x=proficiency, y=greymatter)) +
  geom_point() +
  xlab("Proficiency score for second language") +
  ylab("Gray-matter density (mm^3/voxel")

# B. Calculate the correlation between second language proficiency and gray matter density
grayCor <- cor.test(gray$greymatter,gray$proficiency)
grayCor$estimate
  
```
C. Test the null hypothesis
We can reject the null hypothesis because the correlation coefficient is greater than 0 (`r grayCor$estimate`). 

D. What are you assumptions for part c?
1) Random Sample from a Normal Population
2) Bivariate Normal Distribution - Bell shaped normal distribution in two dimensions

E. Does the scatter plot support these assumptions?
The scatter plot indicates a bivariate normal distribution where the relationship between X and Y is linear, cloud of points have a ellipitcal shape, and the frequency distribution of X and Y separately appear normal



###Questions 19
```{r C16_Q19, echo = TRUE}
#Load Data
rat <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter16/chap16q19LiverPreparation.csv"))

# A. Calculate the correlation coefficient between the taurocholate unbound fraction and the concentration
ratCor <- cor.test(rat$unboundFraction,rat$concentration)
ratCor$estimate

# B. Plot the relationship between the two variables in a graph
ggplot(data = rat, aes(x = concentration, y = unboundFraction)) + 
  geom_point() +
  xlab("Concentration (microMole)") +
  ylab("Unbound Fraction")

```
C. Examine the plot in part (b). The relationship appears to be maximally strong, yet the correlation coefficient you calculated in part (a) is not near the maximum possible value. Why not? 
It doesn't near maximum because it is not a linear relationship. 

D. What steps would you take with these data to meet the assumptions of correlation analysis? I would transform the data with an arcsine transformation because we are working with fractions. 

## 2. Correlation SE

Consider the following dataset:

```{r make_data, echo = TRUE}
set.seed(20181011)
library(mnormt)

mat <- rmnorm(10, varcov = matrix(c(1,0.3, 0.3, 1), ncol=2)) %>%
  round(2) %>%
  as.data.frame %>%
  rename(cats = "V1", happiness_score = "V2")

knitr::kable(mat, "html") %>% kableExtra::kable_styling("striped")

```

### 2a.
Are these two variables correlated? What is the output of `cor()` here. What does a test show you?

```{r, echo = TRUE}
matCor <- cor.test(mat$cats, mat$happiness_score)
```


### 2b.
What is the SE of the correlation based on the info from `cor.test()`
```{r, echo = TRUE}
provided <- (0.7877084 - 0.3136313 )/2
provided
se_man <- sqrt((1-(matCor$estimate^2))/(length(mat$cats-2)))
se_man
```

### 2c.
Now, what is the SE via simulation? To do this, you'll need to use `cor()` and get the relevant parameter from the output (remember - you get a matrix back, so, what's the right index!), `replicate()`, and `sample()` or `dplyr::sample_n()` with `replace=TRUE` to get, let's say, 1000 correlations. How does this compare to your value above?

```{r, echo = TRUE}
ses <- replicate(1000, cor(sample_n(mat, nrow(mat), replace=TRUE))[1,2])

sd(ses)

#Similar - a hair larger.

```


## 3. W&S Chapter 17

###Questions 19, 30, 31
```{r C17_Q19, echo = TRUE}
#Load Data
grassland <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter17/chap17q19GrasslandNutrientsPlantSpecies.csv"))

# A. Plot Scatter Plot
plot(species ~ nutrients, data = grassland)

ggplot(grassland, aes(nutrients, species)) +
  geom_point() +
  xlab("Number of Nutrients Added") +
  ylab("Number of Plant Species")

# B. What is the rate of change in the number of plant species supported per nutrient type added? Provide the standard error for your estimate
grasslandReg <- lm(species~nutrients, data = grassland)
# Rate of change in the number of plant species supported per nutrient type added?
coefGrass <- coef(grasslandReg)

# Standard error for your estimate
summary(grasslandReg)
grassSum <- tidy(grasslandReg)

# C. Add the least squares regression line to your scatter plot. What fraction of the variation in the number of plant species is "explained" by the number of nutrients added?
grassFrame <- data.frame(coefGrass)

ggplot(grassland, aes(nutrients, species)) + 
  geom_point() +
  geom_abline(data = grassFrame, aes(slope = grassFrame$coefGrass[2], intercept = grassFrame$coefGrass[1]), alpha = 0.5 )


```

A. Species is the response variables because we are determining if the number of nutrients impacts the number of species present 

B. Rate of change is `r grassSum$estimate[2]`. The standard error is `r grassSum$std.error[2]`

C. The fraction explained by the number of nutrients added is `r summary(grasslandReg)$r.squared`. 

D. Test the null hypothesis for no treatment effect on the number of plant species.

With a test statistic of `r grassSum$statistic[2]` and a p-value of `r grassSum$p.value[2]`, the p-value is less than the 5% significance and we can reject the null hypothesis. 


```{r C17_Q30, echo = TRUE}
#Load Data
teeth <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter17/chap17q30NuclearTeeth.csv"))

# Calculate Regression Line
teethReg <- lm(dateOfBirth~deltaC14, data = teeth)
summary(teethReg)
teethSum <- tidy(teethReg)

# Plot Data and Linear Regression
teeth_fit_plot <- ggplot(data=teeth) +
  aes(x=deltaC14, y=dateOfBirth) +
  geom_point() +
  stat_smooth(method="lm")

teeth_fit_plot

#Calculate Confidence Intervals and Prediction Intervals
predFrame <- data.frame(deltaC14 = min(teeth$deltaC14):max(teeth$deltaC14))
predVals <- predict(teethReg, newdata=predFrame, interval="prediction")
predFrame <- cbind(predFrame, predVals) %>%
  rename(dateOfBirth = fit)

# Plot Prediction Intervals
teeth_fit_plot +
  stat_smooth(method="lm", color="blue") +
  geom_ribbon(data=predFrame, mapping=aes(x=deltaC14, ymin=lwr, ymax=upr),
              fill="grey", alpha=0.6) +
  theme_bw()


```
A. We have a negative relationship with an approximate slope is `r teethSum$estimate[2]`. 

B. Which pair of lines shows the confidence bands? What do these confidence bands tell us? 
The inner bands shows the confidence bands and the confidence bands measures the precision of the predicted year mean for a given deltaC14. 

C. Which pair of lines shows the prediction interval? What does this prediction interval tell us? The outer bands shows the prediction intervals and the prediction interval measures the precision of a predicted single year for a given deltaC14. Thus, the interval brackets most of the data points because it includes the varibility in years from individual year to individual year at a given deltaC14. 


```{r C17_Q31, echo = TRUE}
#Load Data
lastSup <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter17/chap17q31LastSupperPortionSize.csv"))

#Computer Linear Regression
lastSupReg <- lm(portionSize~year, data = lastSup)

#ANOVA test of zero regression slope 
testZero <- anova(lastSupReg)
Fvalue <- testZero$`F value`[1]
summary(lastSupReg)
lastSupSum <- tidy(lastSupReg)

# Plot Linear Regression
lastSup_fit_plot <- ggplot(data=lastSup) +
  aes(x=year, y=portionSize) +
  geom_point() +
  stat_smooth(method="lm")

lastSup_fit_plot

# Predict Linear Regression
predFrame <- data.frame(year = min(lastSup$year):max(lastSup$year))
predVals <- predict(lastSupReg, newdata=predFrame, interval="prediction")

predFrame <- cbind(predFrame, predVals) %>%
  rename(portionSize = fit)

head(predFrame)

# Plot Prediction with Ribbon
lastSup_fit_plot +
  stat_smooth(method="lm", color="blue") +
  geom_ribbon(data=predFrame, mapping=aes(x=year, ymin=lwr, ymax=upr),
              fill="grey", alpha=0.6) +
  theme_bw()

# Plot Residuals
plot(lastSupReg, which=1)

# Plot Histogram of Residuals
hist(residuals(lastSupReg))

```
A. Calculate a regression line that best describes the relationship between year of painting and the portion size. What is the trend? How rapidly has portion size changed in paintings? 
Positive trend. The portion size per year has changed `r lastSupSum$estimate[2]`. 

B. What is the plausibe range of value for the slope of this relationship? Calculate a 95% confidence interval. 
`r confint(lastSupReg)[2,]`.

C. Test for a change in relative portion size painted in these works of art with the year in which they were painted. 
Null Hypothesis: No change in relative portion size painted in these works of art with the year in which they were painted. 

With a test statistic of `r lastSupSum$statistic` and a p value `r lastSupSum$p.value`, we can say that there was a change in relative portion size with the year they were painted in. In addition, the ANOVA table shows us that the F value is `r Fvalue` and if the null hypothesis is true, it would be zero. 

D. Draw a residual plot of these data and examine it carefully. Can you see any cause for concern about using a linear regression? Suggest an approach that could be tried to address the problem. 
The residual plot does not normality and equal variance assumptions because there is a noticeable dip along the x-axis, unequal variance above and below the line at all values of x, and there appear to be higher density points away from the line. You can transfrom the data or apply a nonlinear regression technique like smoothing. I think a spline fit would do well in this occassion. 


## 4. Intervals and simulation

Fit the deet and bites model from lab.

```{r, echo = TRUE}
deet <- read.csv(url("http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter17/chap17q29DEETMosquiteBites.csv"))

# Plot Data
ggplot(data = deet, aes(x = dose, y = bites)) +
  geom_point() +
  stat_smooth(method = lm) +
  xlab("Dose") +
  ylab("Bites")

```

Now, look at `vcov()` applied to your fit. For example:

```{r, echo = TRUE}
# Linear Regression for Bites to Deet Dosage
deetReg <- lm(bites ~ dose, data = deet)

# Variance 
vcov(deetReg)
```

What you have here is the variance-covariance matrix of the parameters of the model. In essence, every time you larger slopes in this case will have smaller intercepts, and vice-verse. This maintains the best fit possible, despite deviations in the slope and intercept.  BUT - what's cool about this is that it also allows us to produce simulations (posterior simulations for anyone interested) of the fit. We can use a package like `mnormt` that let's us draw from a multivariate normal distribution when provided with a vcov matrix.  For example...


```{r mnorm, echo=TRUE}
# Load mnormt package
library(mnormt)

# Draw a multivariate normal distribution
rmnorm(4, mean = coef(deetReg), varcov = vcov(deetReg))
```

produces a number of draws of the variance and the covariance!

### 4a. Fit simulations!
Using `geom_abline()` make a plot that has the following layers and shows that these simulated lines match up well with the fit CI. 1) the data, 2) the lm fit with a CI, and 3) simulated lines. You might have to much around to make it look as good as possible.


```{r, echo = TRUE}
coefSims <- rmnorm(500, mean = coef(deetReg), varcov = vcov(deetReg)) %>%
  as.data.frame

ggplot(deet, aes(dose, bites)) +
  geom_point() +
  geom_abline(data = coefSims, aes(slope = dose, intercept = `(Intercept)`), alpha = 0.5) +
  stat_smooth(data = deet, method=lm, fill = "blue")
```

### 4b. Prediction simulations!

That's all well and good, but what about the prediction intervals? To each line, we can add some error drawn from the residual standard deviation. That residual can either be extracted from `summary()` or you can get the `sd` of `residuals`.

Now, visualize the simulated prediction interval around the fit versus the calculated prediction interval around the fit via `predict`. **+1 extra credit for a clever visualization of all elements on one figure - however you would like**

```{r, echo = TRUE}
coefSims <- coefSims %>%
  mutate(error = rnorm(n(), 0, sd(deetReg$residuals)))

pred_frame <- predict(deetReg, interval="prediction") %>% cbind(deet)

ggplot(deet, aes(dose, bites)) +
  geom_point() +
  geom_abline(data = coefSims, aes(slope = dose, intercept = error+`(Intercept)`), alpha = 0.5, color = "orange") +
  geom_ribbon(data = pred_frame, aes(ymin = lwr, ymax = upr), fill = "purple", alpha = 0.5) +
  geom_abline(data = coefSims, aes(slope = dose, intercept = `(Intercept)`), alpha = 0.5) +
  stat_smooth(data = deet, method=lm, fill = "blue")

```